[{"content":"task failed case taskEnd: SparkListenerTaskEnd =\u0026gt; listener.onTaskEnd(taskEnd) task failed reson\noom 避免调度到之前申请的executor，需要重新申请新的大内存executor （维护一个单独的executor集合） 其他失败原因，能够解决 org.apache.spark.resource.ResourceProfile#getResourcesForClusterManager 获取executor 资源\n附录 参考文献 ","date":"2024-09-09T11:12:15+08:00","permalink":"https://sanshuil.github.io/post/executor_oom/","title":"Executor_oom"},{"content":"kyuubi 异常 8月10日 22:43:06 1190188 server 监控异常\n查看日志，连接vms172729异常\n发现机器日志大量输出Too many open files 日志\n24/08/10 23:15:52,016 [KyuubiTBinaryFrontend: Thread-31] WARN TThreadPoolServer: Transport error occurred during acceptance of message. org.apache.thrift.transport.TTransportException: java.net.SocketException: Too many open files (Accept failed) at org.apache.thrift.transport.TServerSocket.acceptImpl(TServerSocket.java:134) ~[libthrift-0.9.3.jar:0.9.3] at org.apache.thrift.transport.TServerSocket.acceptImpl(TServerSocket.java:35) ~[libthrift-0.9.3.jar:0.9.3] at org.apache.thrift.transport.TServerTransport.accept(TServerTransport.java:60) ~[libthrift-0.9.3.jar:0.9.3] at org.apache.thrift.server.TThreadPoolServer.serve(TThreadPoolServer.java:162) ~[libthrift-0.9.3.jar:0.9.3] at org.apache.kyuubi.service.TBinaryFrontendService.$anonfun$run$2(TBinaryFrontendService.scala:102) ~[kyuubi-common_2.12-1.6.1-incubating.jar:1.6.1-incubating] at org.apache.kyuubi.service.TBinaryFrontendService.$anonfun$run$2$adapted(TBinaryFrontendService.scala:102) ~[kyuubi-common_2.12-1.6.1-incubating.jar:1.6.1-incubating] at scala.Option.foreach(Option.scala:407) ~[scala-library-2.12.15.jar:?] at org.apache.kyuubi.service.TBinaryFrontendService.run(TBinaryFrontendService.scala:102) ~[kyuubi-common_2.12-1.6.1-incubating.jar:1.6.1-incubating] at java.lang.Thread.run(Thread.java:745) ~[?:1.8.0_121] Caused by: java.net.SocketException: Too many open files (Accept failed) at java.net.PlainSocketImpl.socketAccept(Native Method) ~[?:1.8.0_121] at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409) ~[?:1.8.0_121] at java.net.ServerSocket.implAccept(ServerSocket.java:545) ~[?:1.8.0_121] at java.net.ServerSocket.accept(ServerSocket.java:513) ~[?:1.8.0_121] at org.apache.thrift.transport.TServerSocket.acceptImpl(TServerSocket.java:129) ~[libthrift-0.9.3.jar:0.9.3] ... 8 more 手动重启后恢复 排查 机器残留大量 server_operation_logs，查看日志是同一user提交大量相似sql，\n用户debug job调用router artnova 执行大量sql查询，kill后server 恢复\nkyuubi会在session close 后删除 operation log，由于这个job 同时开了多个session，每个session执行大量的sql，使得server open文件数过多，存在大量server_operation_logs，最终异常\n复现 构造多个session，每个执行大量sql\nimport java.sql._ val url = \u0026#34;jdbc:hive2://hive-zk1.hadoop.ctripcorp.com,hive-zk2.hadoop.ctripcorp.com,hive-zk3.hadoop.ctripcorp.com/default;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=kyuubi_adhoc_test_16#kyuubi.engine.type=SPARK_SQL\u0026#34; val open = DriverManager.getConnection(\u0026#34;url\u0026#34;) val open1 = DriverManager.getConnection(\u0026#34;url\u0026#34;) val open2 = DriverManager.getConnection(\u0026#34;url\u0026#34;) val open3 = DriverManager.getConnection(\u0026#34;url\u0026#34;) val open4 = DriverManager.getConnection(\u0026#34;url\u0026#34;) (1 to 20000).map{ a =\u0026gt; open.createStatement().executeQuery(s\u0026#34;select $a\u0026#34;) open1.createStatement().executeQuery(s\u0026#34;select $a\u0026#34;) open2.createStatement().executeQuery(s\u0026#34;select $a\u0026#34;) open3.createStatement().executeQuery(s\u0026#34;select $a\u0026#34;) open4.createStatement().executeQuery(s\u0026#34;select $a\u0026#34;) } server 复现 [hive@VMS174752 kyuubi]\n后续 完善zeus 监控 kyuubi server job，从zk获取server node，遍历进行查询，查询超时 限制单个session 执行sql最大数量 支持operation log 关闭 附录 参考文献 ","date":"2024-08-12T14:55:49+08:00","permalink":"https://sanshuil.github.io/post/toomanyfile/","title":"Toomanyfile"},{"content":"job 耗时异常 0807 job 耗时异常 查看执行计划，卡在bhj join，有长尾task，部分task 耗时较长\n对比之前job，执行计划都为bhj，但是task数量比较少，\n耗时异常job application_1716531657697_9775710 正常job application_1716531657697_9665729 对比bhj执行计划异常job多了一次shuffle read\n测试机器 重跑sql，发现执行计划为smj，耗时正常\n对比spark conf，zeus job 配置了spark.sql.adaptive.autoBroadcastJoinThreshold参数\naqe bhj 优化 aqe 参数\nspark.sql.adaptive.autoBroadcastJoinThreshold 在aqe开启时生效，可以根据表run time stats动态选择是否使用bhj\nspark3之前做过优化，计算stats时候会进行裁剪优化Spark3分区裁剪优化, 在这个case，因为表分区大小比spark.sql.autoBroadcastJoinThreshold大\n所以spark默认使用smj，会使用shuffle exchange read，aqe优化为bhj的时候不会消除这个shuffle read，所以导致并行度降低，速度变慢\n解决方案 因为job使用bhj也没有oom，调整spark.sql.autoBroadcastJoinThreshold参数，让job 初始计划使用bhj，避免shuffle read\n输出smj -\u0026gt; bhj 的日志，\n附录 select a.createtime , a.version , a.hotelid , a.masterhotelid , a.roomid , a.effectdate , a.startdate , a.enddate , a.canoverflow , a.reservetime , if(a.restorable = \u0026#39;true\u0026#39;,\u0026#39;T\u0026#39;,if(a.restorable = \u0026#39;false\u0026#39;,\u0026#39;F\u0026#39;,a.restorable)) as restorable , if(a.freesale = \u0026#39;true\u0026#39;,\u0026#39;T\u0026#39;,if(a.freesale = \u0026#39;false\u0026#39;,\u0026#39;F\u0026#39;,a.freesale)) as freesale , a.isdel_htl , a.lst_upd_time , a.data_src , a.datachangelasttime , a.inland from dwhtl.edw_prd_room_quantitypolicy a left join ( select distinct createtime, version, masterhotelid, hotelid, isdel_htl from dwhtl.edw_prd_room_quantitypolicy_base b where d = \u0026#39;2024-08-07\u0026#39; ) b on cast(a.hotelid as string)= cast(b.hotelid as string) where a.d = \u0026#39;2024-08-06\u0026#39; and a.hotelid\u0026gt;0 and a.effectdate \u0026gt;= \u0026#39;2024-08-06\u0026#39; and ((b.version is not null and a.version is not null and concat(a.version,a.createtime) \u0026gt; concat(b.version,b.createtime)) or (b.version is not null and a.version is null and a.createtime \u0026gt; b.createtime) ) ","date":"2024-08-08T11:09:53+08:00","permalink":"https://sanshuil.github.io/post/bhj/","title":"Bhj"},{"content":"zookeeper session # kyuubi 建立 session 24/06/19 14:55:35,984 [main-SendThread(hive-zk3.hadoop.ctripcorp.com:2181)] INFO ClientCnxn: Session establishment complete on server hive-zk3.hadoop.ctripcorp.com/10.63.66.150:2181, sessionid = 0xff902365d9b70e63, negotiated timeout = 60000 # session 被close 24/06/25 19:56:12,970 [main-SendThread(hive-zk3.hadoop.ctripcorp.com:2181)] INFO ClientCnxn: Unable to read additional data from server sessionid 0xff902365d9b70e63, likely server has closed socket, closing socket connection and attempting reconnect 24/06/25 19:56:12,979 [main-EventThread] WARN ZookeeperDiscoveryClient: This Kyuubi instance vms174752.hadoop.sh5.ctripcorp.com:11119 (znode /kyuubi_adhoc_test_appid/serviceUri=vms174752.hadoop.sh5.ctripcorp.com:11119;version=1.6.1-incubating;sequence=0000000006) is now de-registered from ZooKeeper. The server will be shut down after the last client session completes. 24/06/25 19:56:13,301 [main-SendThread(hive-zk1.hadoop.ctripcorp.com:2181)] INFO ClientCnxn: Opening socket connection to server hive-zk1.hadoop.ctripcorp.com/10.63.66.148:2181. Will not attempt to authenticate using SASL (unknown error) 24/06/25 19:56:13,302 [main-SendThread(hive-zk1.hadoop.ctripcorp.com:2181)] INFO ClientCnxn: Socket connection established to hive-zk1.hadoop.ctripcorp.com/10.63.66.148:2181, initiating session 24/06/25 19:56:13,303 [main-SendThread(hive-zk1.hadoop.ctripcorp.com:2181)] WARN ClientCnxn: Unable to reconnect to ZooKeeper service, session 0xff902365d9b70e63 has expired 24/06/25 19:56:13,303 [main-SendThread(hive-zk1.hadoop.ctripcorp.com:2181)] INFO ClientCnxn: Unable to reconnect to ZooKeeper service, session 0xff902365d9b70e63 has expired, closing socket connection 24/06/25 19:56:21,079 [main-EventThread] ERROR ZookeeperDiscoveryClient: Failed to close the persistent ephemeral znodenull java.io.IOException: org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /kyuubi_adhoc_test_appid/serviceUri=vms174752.hadoop.sh5.ctripcorp.com:11119;version=1.6.1-incubating;sequence=0000000006 at org.apache.curator.framework.recipes.nodes.PersistentNode.close(PersistentNode.java:296) ~[curator-recipes-2.12.0.jar:?] at org.apache.kyuubi.ha.client.zookeeper.ZookeeperDiscoveryClient.deregisterService(ZookeeperDiscoveryClient.scala:255) ~[kyuubi-ha_2.12-1.6.1-incubating.jar:1.6.1-incubating] at org.apache.kyuubi.ha.client.zookeeper.ZookeeperDiscoveryClient$DeRegisterWatcher.process(ZookeeperDiscoveryClient.scala:417) ~[kyuubi-ha_2.12-1.6.1-incubating.jar:1.6.1-incubating] at org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:62) ~[curator-framework-2.12.0.jar:?] at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:533) ~[zookeeper-3.4.14.jar:3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf] at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:508) ~[zookeeper-3.4.14.jar:3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf] Caused by: org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /kyuubi_adhoc_test_appid/serviceUri=vms174752.hadoop.sh5.ctripcorp.com:11119;version=1.6.1-incubating;sequence=0000000006 at org.apache.zookeeper.KeeperException.create(KeeperException.java:130) ~[zookeeper-3.4.14.jar:3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf] at org.apache.zookeeper.KeeperException.create(KeeperException.java:54) ~[zookeeper-3.4.14.jar:3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf] at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:882) ~[zookeeper-3.4.14.jar:3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf] at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:250) ~[curator-framework-2.12.0.jar:?] at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:244) ~[curator-framework-2.12.0.jar:?] at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109) ~[curator-client-2.12.0.jar:?] at org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:241) ~[curator-framework-2.12.0.jar:?] at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:225) ~[curator-framework-2.12.0.jar:?] at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:35) ~[curator-framework-2.12.0.jar:?] at org.apache.curator.framework.recipes.nodes.PersistentNode.deleteNode(PersistentNode.java:347) ~[curator-recipes-2.12.0.jar:?] at org.apache.curator.framework.recipes.nodes.PersistentNode.close(PersistentNode.java:291) ~[curator-recipes-2.12.0.jar:?] ... 5 more ### zk 日志 2024-06-25 19:56:12,968 [myid:3] - INFO [ProcessThread(sid:3 cport:-1)::PrepRequestProcessor@494] - Processed session termination for sessionid: 0xff902365d9b70e63 2024-06-25 19:56:12,969 [myid:3] - INFO [CommitProcessor:3:NIOServerCnxn@1001] - Closed socket connection for client /10.63.0.235:47752 which had sessionid 0xff902365d9b70e63 附录 参考文献 ","date":"2024-06-26T14:09:37+08:00","permalink":"https://sanshuil.github.io/post/zookeeper/","title":"Zookeeper"},{"content":"According to the Scala doc a Stream is memoized only as long as something holds on to the head. Using def is one way to avoid that. So the real question is: Does Stream.iterate()() hold on to its head if the only thing returned is the final iteration? (Which, BTW, can be directly indexed rather than take(n).last).\n附录 参考 https://stackoverflow.com/questions/45649044/scala-stream-iterate-and-memory-management\n","date":"2024-05-22T10:08:55+08:00","permalink":"https://sanshuil.github.io/post/scala_leak_mem/","title":"Scala_leak_mem"},{"content":"scala iter continually 介绍 在 Scala 中，可以使用Iterator.continually方法来创建一个持续生成元素的迭代器。如果需要对第一个元素进行特殊处理，可以使用zipWithIndex方法来获取元素的索引，并根据索引来判断是否为第一个元素。\n下面是一个示例代码：\nval iterator = Iterator.continually(\u0026#34;元素\u0026#34;).zipWithIndex.map { case (element, index) =\u0026gt; if (index == 0) { // 对第一个元素进行特殊处理 // 返回处理后的结果 } else { // 对其他元素进行正常处理 // 返回处理后的结果 } } 在上面的代码中，Iterator.continually(\u0026ldquo;元素\u0026rdquo;)会创建一个不断重复生成字符串 \u0026ldquo;元素\u0026rdquo; 的迭代器。然后使用zipWithIndex方法获取元素的索引，并根据索引来判断是否为第一个元素。如果是第一个元素，则进行特殊处理；否则进行正常处理。\nkyuubi trino engine 的处理 trino engine 支持使用流式的方式拉数据\ndef execute(): Iterator[List[Any]] = { Iterator.continually { @tailrec def getData(): (Boolean, List[List[Any]]) = { if (trino.isRunning) { val data = trino.currentData().getData() trino.advance() if (data != null) { (true, data.asScala.toList.map(_.asScala.toList)) } else { getData() } } else { ....... updateTrinoContext() (false, List[List[Any]]()) } } getData() } .takeWhile(_._1) .flatMap(_._2) } //executeStatement increment mode val resultSet = trinoStatement.execute() iter = FetchIterator.fromIterator(resultSet) 使用Iterator.continually方法来创建一个持续获取data的过程，在increment mode时会流式的拉取数据。takeWhile方法会一直获取数据，直到获取到_.1 为false为止。\n显式调用hasNext，会阻塞，直到trino client sql语句执行完成。client 可以获取到更多runing info\n附录 [TRINO] Trino engine improve operation log\n","date":"2024-05-13T11:38:55+08:00","permalink":"https://sanshuil.github.io/post/scala-iter-continually/","title":"Scala iter continually"},{"content":"kyuubi engine pool select policy 目前有两种策略random 和 poll 策略\nval ENGINE_POOL_SELECT_POLICY: ConfigEntry[String] = buildConf(\u0026#34;kyuubi.engine.pool.selectPolicy\u0026#34;) .doc(\u0026#34;The select policy of an engine from the corresponding engine pool engine for \u0026#34; + \u0026#34;a session. \u0026lt;ul\u0026gt;\u0026#34; + \u0026#34;\u0026lt;li\u0026gt;RANDOM - Randomly use the engine in the pool\u0026lt;/li\u0026gt;\u0026#34; + \u0026#34;\u0026lt;li\u0026gt;POLLING - Polling use the engine in the pool\u0026lt;/li\u0026gt;\u0026#34; + = \u0026#34;\u0026lt;/ul\u0026gt;\u0026#34;) .version(\u0026#34;1.7.0\u0026#34;) .stringConf .transformToUpperCase .checkValues(Set(\u0026#34;RANDOM\u0026#34;, \u0026#34;POLLING\u0026#34;)) .createWithDefault(\u0026#34;RANDOM\u0026#34;) 根据负载来选择engine 针对spark engine， 可以收集到spark engine 当前的task数和opensession数。\n首先，spark engine 定时将task、session输出到zk上 kyuubi server选择engine时，从zk上获取engine的task数和session数，然后根据负载选择engine 设置session阈值，当session数超过阈值时，创建新的engine ","date":"2024-04-24T10:36:45+08:00","permalink":"https://sanshuil.github.io/post/kyuubi_engine_adaptive_pool_size/","title":"Kyuubi_engine_adaptive_pool_size"},{"content":"apache httpClent 使用时出现异常 Caused by: org.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.leaseConnection(PoolingHttpClientConnectionManager.java:316) ~[httpclient-4.5.13.jar:4.5.13] at org.apache.http.impl.conn.PoolingHttpClientConnectionManager$1.get(PoolingHttpClientConnectionManager.java:282) ~[httpclient-4.5.13.jar:4.5.13] at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:190) ~[httpclient-4.5.13.jar:4.5.13] at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186) ~[httpclient-4.5.13.jar:4.5.13] at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89) ~[httpclient-4.5.13.jar:4.5.13] at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) ~[httpclient-4.5.13.jar:4.5.13] at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185) ~[httpclient-4.5.13.jar:4.5.13] at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83) ~[httpclient-4.5.13.jar:4.5.13] at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:108) ~[httpclient-4.5.13.jar:4.5.13] at org.apache.kyuubi.plugin.DscAuthPlugin$.doDscAuth(DscAuthPlugin.scala:78) ~[kyuubi-server_2.12-1.6.1-incubating.jar:1.6.1-incubating] ... 24 more httpClient的创建时候使用默认的PoolingHttpClientConnectionManager\nprivate val httpClient = HttpClientBuilder.create().setDefaultRequestConfig(requestConfig).build() org.apache.http.impl.conn.PoolingHttpClientConnectionManager#setDefaultMaxPerRoute //default value 2 org.apache.http.impl.conn.PoolingHttpClientConnectionManager#getMaxPerRoute PoolingHttpClientConnectionManager的配置中有两个最大连接数量，分别控制着总的最大连接数量和每个route的最大连接数量，最大路由连接数MaxPerRoute默认为2，总连接数量MaxTotal默认为20。每次新来一个请求，如果连接池中已经存在相同route并且可用的connection，连接池就会直接复用这个connection，当不存在route相同的connection，就会新建一个connection为之服务；如果连接池已满，则请求会等待直到被服务或者超时。每次读取response会释放connection\n消费resp，此时会close，release conn\nat org.apache.http.impl.execchain.ConnectionHolder.releaseConnection(ConnectionHolder.java:120) at org.apache.http.impl.execchain.ResponseEntityProxy.releaseConnection(ResponseEntityProxy.java:76) at org.apache.http.impl.execchain.ResponseEntityProxy.streamClosed(ResponseEntityProxy.java:145) at org.apache.http.conn.EofSensorInputStream.checkClose(EofSensorInputStream.java:228) at org.apache.http.conn.EofSensorInputStream.close(EofSensorInputStream.java:172) at org.apache.http.client.entity.LazyDecompressingInputStream.close(LazyDecompressingInputStream.java:97) at org.apache.http.util.EntityUtils.consume(EntityUtils.java:90) 正常情况下，每个connection会在连接后释放，但是对dsc 鉴权处理有问题，当code不为200时，未消费response\ncode 为200时，使用jsckson处理entity，会close response\nval content = objectMapper.readTree(response.getEntity.getContent) at org.apache.http.impl.execchain.ConnectionHolder.releaseConnection(ConnectionHolder.java:120) at org.apache.http.impl.execchain.ResponseEntityProxy.releaseConnection(ResponseEntityProxy.java:76) at org.apache.http.impl.execchain.ResponseEntityProxy.streamClosed(ResponseEntityProxy.java:145) at org.apache.http.conn.EofSensorInputStream.checkClose(EofSensorInputStream.java:228) at org.apache.http.conn.EofSensorInputStream.close(EofSensorInputStream.java:172) at java.util.zip.InflaterInputStream.close(InflaterInputStream.java:227) at java.util.zip.GZIPInputStream.close(GZIPInputStream.java:136) at org.apache.http.client.entity.LazyDecompressingInputStream.close(LazyDecompressingInputStream.java:94) at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._closeInput(UTF8StreamJsonParser.java:242) at com.fasterxml.jackson.core.base.ParserBase.close(ParserBase.java:347) at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:2994) at com.fasterxml.jackson.databind.ObjectMapper.readTree(ObjectMapper.java:1737) code不为200时抛出异常，response 未close\n24/01/17 10:03:08,099 [KyuubiTBinaryFrontendHandler-Pool: Thread-8956845] ERROR KyuubiTBinaryFrontendService: Error executing statement: org.apache.kyuubi.KyuubiException: Do dsc auth failed, http code: 504 at org.apache.kyuubi.plugin.DscAuthPlugin$.doDscAuth(DscAuthPlugin.scala:89) ~[kyuubi-server_2.12-1.6.1-incubating.jar:1.6.1-incubating] at org.apache.kyuubi.operation.ExecuteStatement.beforeRun(ExecuteStatement.scala:60) ~[kyuubi-server_2.12-1.6.1-incubating.jar:1.6.1-incubating] at org.apache.kyuubi.operation.AbstractOperation.run(AbstractOperation.scala:162) ~[kyuubi-common_2.12-1.6.1-incubating.jar:1.6.1-incubating] at org.apache.kyuubi.session.AbstractSession.runOperation(AbstractSession.scala:99) ~[kyuubi-common_2.12-1.6.1-incubating.jar:1.6.1-incubating] at org.apache.kyuubi.session.KyuubiSessionImpl.runOperation(KyuubiSessionImpl.scala:191) ~[kyuubi-server_2.12-1.6.1-incubating.jar:1.6.1-incubating] at org.apache.kyuubi.session.AbstractSession.$anonfun$executeStatement$1(AbstractSession.scala:129) ~[kyuubi-common_2.12-1.6.1-incubating.jar:1.6.1-incubatin 查询日志，2次504 code，connection pool 被占满，后续连接抛出异常\n验证，未消费Resp，http 请求失败\npublic static void main(String[] args) throws IOException, InterruptedException { CloseableHttpClient httpclient = HttpClients.createDefault(); HttpGet httpGet = new HttpGet(\u0026#34;https://www.baidu.com\u0026#34;); CloseableHttpResponse response1 = httpclient.execute(httpGet); System.out.println(response1.getStatusLine()); CloseableHttpResponse response2 = httpclient.execute(httpGet); System.out.println(response2.getStatusLine()); CloseableHttpResponse response3 = httpclient.execute(httpGet); System.out.println(response3.getStatusLine()); } 使用httpClient时，最好每次消费Response， 最后 close response\nREF https://hc.apache.org/httpcomponents-client-4.5.x/quickstart.html\n// The underlying HTTP connection is still held by the response object // to allow the response content to be streamed directly from the network socket. // In order to ensure correct deallocation of system resources // the user MUST call CloseableHttpResponse#close() from a finally clause. // Please note that if response content is not fully consumed the underlying // connection cannot be safely re-used and will be shut down and discarded // by the connection manager.\n","date":"2024-04-19T10:24:07+08:00","permalink":"https://sanshuil.github.io/post/httpclient%E8%B8%A9%E5%9D%91/","title":"HttpClient踩坑"}]